<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Deepgram => LLM => ElevenLabs TTS</title>
</head>
<body>
  <button id="startBtn">Start</button>
  <button id="stopBtn">Stop</button>

  <h3>Transcripts (Partial / Final):</h3>
  <textarea id="transcriptLog" rows="10" cols="50" readonly></textarea>

  <h3>LLM Responses:</h3>
  <textarea id="agentOutput" rows="10" cols="50" readonly></textarea>

  <script>
    let mediaRecorder, ws;

    const transcriptBox = document.getElementById("transcriptLog");
    const agentBox = document.getElementById("agentOutput");

    function appendTranscript(line) {
      transcriptBox.value += line + "\n";
    }
    function appendLLMResponse(line) {
      agentBox.value += line + "\n";
    }

    document.getElementById('startBtn').onclick = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream);

        // Setup WS
        ws = new WebSocket(`ws://${window.location.host}/stt`);
        // we expect binary data (MP3) as well => set
        ws.binaryType = 'arraybuffer';

        ws.onopen = () => {
          appendTranscript("WS connected /stt");
        };
        ws.onclose = () => {
          appendTranscript("WS closed");
        };

        // Receiving partial transcripts, final transcripts, or TTS audio
        ws.onmessage = async (evt) => {
          if (typeof evt.data === "string") {
            // Probably JSON
            try {
              const msg = JSON.parse(evt.data);
              if (msg.transcript) {
                if (msg.is_final) {
                  appendTranscript("[Final] " + msg.transcript);
                } else {
                  appendTranscript("[Partial] " + msg.transcript);
                }
              }
              if (msg.llmResponse) {
                appendLLMResponse("LLM says: " + msg.llmResponse);
              }
            } catch (err) {
              console.log("Got text that's not JSON:", evt.data);
            }
          } else if (evt.data instanceof ArrayBuffer) {
            // We likely got MP3 audio from ElevenLabs
            console.log("Received TTS audio chunk (MP3):", evt.data.byteLength, "bytes");
            await playMp3(evt.data);
          }
        };

        // chunk mic data every 300ms
        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0 && ws.readyState === WebSocket.OPEN) {
            e.data.arrayBuffer().then((buff) => {
              ws.send(buff);
            });
          }
        };

        mediaRecorder.start(300);
        appendTranscript("Recording started...");
      } catch (err) {
        appendTranscript("Error: " + err);
      }
    };

    document.getElementById('stopBtn').onclick = () => {
      if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
        appendTranscript("Recording stopped.");
      }
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.close();
      }
    };

    async function playMp3(arrayBuff) {
      try {
        const audioCtx = new AudioContext();
        // decode mp3 from arrayBuffer
        const audioData = await audioCtx.decodeAudioData(arrayBuff);
        const source = audioCtx.createBufferSource();
        source.buffer = audioData;
        source.connect(audioCtx.destination);
        source.start();
      } catch (err) {
        console.error("Error decoding MP3 arrayBuffer:", err);
      }
    }
  </script>
</body>
</html>
